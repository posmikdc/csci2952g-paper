\documentclass[twoside,11pt]{article}

\usepackage{style}
\usepackage[numbers]{natbib}
\setcitestyle{numbers,square}
\ShortHeadings{
%% short title here
Literature Review (CSCI2952G)
}{
% last name
Narayanan, Posmik, Tran}
\firstpageno{1}

\begin{document}

\title{	Literature Review (CSCI2952G) \\
\vspace{.1in}
An Overview of Graph Neural Networks, Protein-Protein Interaction, and Manifold Graph Embedding 	
}

\author{Charumathi Narayanan, Daniel Posmik, Minh Le Tran}

\maketitle
\date{4 }

\section{Protein-Protein Interactions} 
\label{sec:Intor}
Proteins are the essential workers in the body. They also rarely act alone, often interacting with many other proteins to perform their task properly. It is these web of interactions that lay the foundation for all the activities and functions ensuring survival of the body (\citet{PPI_hist}). These protein-protein interactions (PPIs) are thus a vital source of information of how the body functions at the cellular level. Moreover, since illness are often caused by mutations and disruption to PPIs, understanding more about PPIs can greatly inform drug development(\citet{PPI_disease}). In research, PPI are often represented graphically, with nodes representing the protein, and edges representing interactions between the two connected nodes (\citet{PPI_methods}). These graphs are then analyzed computationally. As biological experiments uncover more proteins and their interactions, there are now many databases for PPIs, including STRING, KEGG, BioGRID, and BioRank(\citet{PPI_methods}). There are also many PPI datasets pertaining to a specific illness, such as OncoPPi, a cancer-focused dataset (\citet{OncoPPi}). However, the biochemical relationships between proteins and the functional consequences of physical interactions remain extremely complicated, given the magnitude and variety of proteins. Due to this complex data, the field of PPIs is a popular area of application for computational analysis. In particular, many deep learning methods can integrate such data for a variety of tasks, including prediction of unknown proteins interaction, or identification of likely protein targets for drug effectiveness (\citet{PPI_DL}). However, as typical of the deep learning field, there is no single best architecture for modeling these PPIs, and more work remains to be able to fully capture and utilize the dataset effectively. 
\section{Deep Learning Architectures for Protein-Protein Interactions}
There are multiple Deep Learning architectural paradigms critical for robust protein feature representation,geometry modeling and evolutionary sequence comprehension. Literature focused on models like Discriminative Network Embedding (DNE) (\citet{yan_etal_2024}) portrays the protein landscape as a large, non-Euclidean network. The primary input is the network topology (who interacts with whom). The core capability is generating robust, low-dimensional network embeddings calculated through contrastive loss that capture holistic and functional relationships, supporting tasks like PPI prediction and functional module identification. Architectures such as ProS-GNN (\citet{wang_etal_2023}) use GNNs that operate directly on the $3D$ molecular structure. Therefore, the $3D$ structure and spatial coordinates is used as input data. The main idea is the use of message-passing schemes to encode local $3D$ spatial information and non-covalent interactions, crucial for accurately predicting physical properties dependent on structural properties, such as changes in thermodynamic stability ($\Delta \Delta G$). The research further develops on these changes to predict insilico mutations. Furthermore, the pLLM paradigm (\citet{xiao_etal_2025}) adapts Transformer-based models from NLP (like BERT etc.). Databases like Uniref/UniParc consisting of billions of linear amino acid sequences are used. The model leverages this data to learn the evolutionary grammar of proteins, which generates highly contextual embeddings that encode latent information about function and structure as well as distal relations, serving as a powerful transfer learning backbone for numerous downstream tasks. As an example, state-of-the-art (SOTA) models like ProteinGPT (\citet{sun_etal_2025}) represent the amalgamation of both sequence and structure. These systems process both sequence (via the pLLM backbone) and structural data (via GNNs or other geometric encoders). The key feature involves introducing adapter or projection layers to align the pLLMs latent space with that of the GNN's embeddings, enabling the resulting large language model to perform downstream tasks based on its comprehensive multi-modal understanding of the protein landscape.
\label{sec:architecture}


\section{Manifold Graph Embedding} \label{sec:manifold}

Manifold estimation is motivated by the manifold hypothesis, which posits that observed data originates from a lower-dimensional data generating process corrupted by high-dimensional noise (\citet{fefferman_etal_2013}, \citet{meng_eloyan_2021}). This technique generalizes non-linear dimensionality reduction while explicitly preserving geometric properties of the data. For graph-structured data, manifold estimation leverages well-known connections between discrete graphs and non-Euclidean embedding space curvature, where characteristic graphs correspond to principle curvaturesâ€”tree-like graphs embed optimally in hyperbolic spaces while chain graphs are best represented in spherical space (\citet{bronstein_etal_2017}, \citet{weber_2018}, \citet{weber_2020}). The learning problem requires estimating the manifold class $\mathcal{M}$, intrinsic dimension $p$, curvature $\kappa$, and projection map $\pi: \mathcal{D} \rightarrow \mathcal{M}_\kappa^p$ where $\mathcal{D} = G = \{V,E\}$, with \citet{lubold_etal_2022} and \citet{meng_eloyan_2021} providing data-driven estimation techniques. Under correct specification, a node's projection $\pi_\mathcal{M}^p(X_{i,j})$ yields information about node importance (\citet{yamada_2025}, \citet{xu_2020}), where highly connected nodes lie in flatter space and isolated nodes lie in highly curved space. The inverse gradient $\hat{w}_{ij} := |\nabla~\pi_\mathcal{M}^p(X_{i,j})|^{-1}$ provides weights that capture geometric, topological, and relational context, making them particularly valuable for high-dimensional unsupervised settings where spectral methods may fail (\citet{baptista_etal_2023}, \citet{rubin-delanchy_2021}). For practical implementation, \citet{digiovanni_etal_2022} propose "soft manifolds" using Riemannian stochastic gradient descent (\citet{bonnabel_2013}), with similar approaches offered by \citet{marinoni_etal_2023} and \citet{jyothish_jannesari_2025}. Refer to Appendix~\ref{sec:app_a} for a more detailed exposition. 

\newpage

\appendix
\section*{Appendix A.} \label{sec:app_a}

Manifold estimation is motivated by the manifold hypothesis \citep{fefferman_etal_2013} which intuitively states that our realized sample (i.e. our data) stems from a lower-dimensional data generating process that is subsequently corrupted by high-dimensional noise (\citet{fefferman_etal_2013}, \citet{meng_eloyan_2021}). We shall refer to the process of reconstructing and estimating this low-dimensional manifold as manifold estimation or manifold embedding (two terms we will use interchangeably). Intuitively, manifold estimation is a generalization of non-linear dimensionality reduction in which we may explicitly preserve geometric properties of our data.  

For high-dimensional noisy graph-structured data, manifold estimation is particularly well-suited to well-known links between discrete graphs and the curvature of non-Euclidian embedding spaces \citep{bronstein_etal_2017}. While no choice of embedding space is perfect due to distortion trade-offs, it can be shown that certain "characteristic graphs" correspond to principle curvatures, e.g., tree-like graphs are optimally embedded in hyperbolic spaces while chain graphs are best represented in spherical space (\citet{weber_2018}, \citet{weber_2020}). 

Manifold estimation is a non-trivial learning problem that requires the careful consideration of trade-offs (e.g., distortion and relational context) and parameter estimation. The parameters that need to be estimated are the class of manifold $\mathcal{M}$, choosing mixed effects structure (e.g., clique-level fixed effects), the intrinsic dimension of the manifold ($p$), and the projection map from the data ($\mathcal{D}$) to the manifold, i.e. 

\[\pi: \mathcal{D} \rightarrow \mathcal{M}_\kappa^p \quad \text{where} \quad \mathcal{D} = G = \{V,E\}\]

Here, $\kappa$ is a measure of curvature and our data $\mathcal{D}$ can be expressed as a graph $G = \{V,E\}$. \citet{lubold_etal_2022} present a data-driven, replicable alternative to ex-ante choosing manifold class ($\mathcal{M}$), dimension ($p$), and curvature ($\kappa$). The technique of \citet{lubold_etal_2022} demonstrate how the connection likelihood has an inverse relationship with the distance of the projected points on the latent manifold. \citet{meng_eloyan_2021} present a regularized principled manifold estimation technique. 

Under correct specification of $\pi(\cdot)$ and the manifold parameters, the key is that for any node $X_{ij} \in G = \{V_X, E_X\}$, its projection on the manifold $\pi_\mathcal{M}^p(X_{i,j})$ of class $\mathcal{M}$ and intrinsic dimension $\mathcal{M}$ gives us valuable information about node importance (e.g., \citet{yamada_2025} use Forman Curvature, \citet{xu_2020} offers a broader discussion). By the above embedding results, highly connected nodes will lie in "flatter" space while more isolated nodes (e.g., the terminal nodes of a tree-graph) will lie in highly curved space. Thus, the inverse of the absolute gradient at the projection $\pi_\mathcal{M}^p(X_{i,j})$, say $\hat{w}_{ij} := |\nabla~\pi_\mathcal{M}^p(X_{i,j})|^{-1}$, will give us insight into how important this node is in the graph. Our goal is to use these inverse weights $w_{ij}$ as an additional feature in protein-protein interaction prediction. These weights $w_{ij}$ lend themselves particularly well to high-dimensional, unsupervised environments since they respect geometric (e.g., curvature, embedding space), topological, and relational contexts of the data. Alternative embedding techniques, e.g., well-established spectral matrix embedding methods, may not readily extend to heterogeneous geometry and high-dimensional settings (for relevant discussions, see \citet{baptista_etal_2023}, \citet{rubin-delanchy_2021}). 

While we have described manifold estimation in theory, we have yet to cover practical considerations. Due to time constraints, we want flexible, out-of-the-box embedding algorithms that can handle high-dimensional, heterogeneous graphs.  \citet{digiovanni_etal_2022} propose "soft manifolds" as a curvature-aware, flexible solution to the heterogeneity problem. In \citet{digiovanni_etal_2022} Proposition 4.1, we are given a numerical embedding procedure based on Riemannian stochastic gradient descent (R-SGD) \citep{bonnabel_2013}. \citet{marinoni_etal_2023} offer a similar implementation. \citet{jyothish_jannesari_2025} offer insight into how manifold embeddings work within a graph transformer framework and provide algorithmic details. 


\newpage 
\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}
