# DLGen Paper Meeting

## Questions

- **Data**: Any advice/pointers?
- **Choosing Scope**: Don't want to overdo it; we only have limited time 
- **Attention weights from graphs**: 

There are papers that link GNNs and transformer architecture (see, e.g., Dwivedi & Bresson (2021)). 

*Idea 1: Manifold Embedding*

The core idea is to use manifolds, low-dimensional representations of potentially high-dimensional noisy graphs, as low-dimensional representations. A first strategy would be the algorithm in Proposition 4.1 in [Giovanni et al. (2022)](https://arxiv.org/pdf/2202.01185). Essentially, this would be the idea: 

[`1`] Embed graph in network (e.g., numerical projection approaches) -> [`2`] For each node $i \in V$, obtain $w_i = 1/\kappa$ where $\kappa$ is a measure of curvature -> [`3`] Use $w_i$ for within attention mechanism for prediction task 

The idea is that the geometry of the graph is in direct relation to how much we care about it. Specifically, a graph's "density" (Think: Node centrality) is inversely related to its curvature in the embedding manifold, thus highly connected nodes may lie in relatively "flat" space. "Outlier nodes" may lie in highly curved space. If we penalize by curvature, we have our desired weighting scheme. 

As opposed to convolutions, this attention mechanism is not only statistically principled but also respects the "network grammar". 

Literature that is closely related to this is "fused" GNN + manifold estimation (notably, [Deng et al. (2023)](https://arxiv.org/abs/2304.01081)). This essentially uses the manifold embeddings as a dictionary for prediction, specifically the regularization component within prediction. 

*Idea 2: Spectral and Graph Methods*

This could be more feasible and would not require any manifold embeddings. The idea would be to use spectral properties of the graph's representation matrix (eigenvectors, Laplacian, etc.) 

## Meeting Notes

- Novelty can come from combination of dataset/ problem and new method; last minute plan: comprehensive benchmarking; explainability  
- Protein models dont capture protein-protein interaction info; each protein 
- Use manifold curvature as feature or additional encoding; similar to positional encoding in text; have interaction info fed into model; 
- Use graph attention Neural network; 
- Use manifold curvature as weights (weighted graph)
- Maybe look into gene-gene interaction; Intuition: positional encoding on top of words gives model sense of grammer (further, closer); in gene: can we do better than positional encoding; 
- TAD vs gene-gene: gene-gene is borader; one gene expresses TF which then activates another gene; interaction is defined very loosely; TAD could be one pathway; pathways and regulation schemes; goes beyond physical contact; 
- Cite the functional data analysis literature on including manifold curvature as covariate 
- String DB (database), for instance, for data 
- Framing this problem in protein-protein space may be easier 
- 1-2 single cell transformer papers; 1-2 protein transformer paper
